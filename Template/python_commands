#! /usr/bin/env python
# -*- coding: utf-8 -*-
from CTCModel import CTCModel
from IPython.display import clear_output
from __future__ import division, print_function
from bs4 import BeautifulSoup
from collections import Counter, defaultdict, deque
from contextlib import closing
from datetime import datetime
from dbfread import DBF
from flask import Flask, render_template, request, redirect, url_for, abort, session, flash
from h5py import File
from itertools import permutations
from keras import Input
from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
from keras.datasets import imdb, mnist
from keras.engine import Model
from keras.layers import Input, Dense, Flatten, SimpleRNN, GRU, LSTM, TimeDistributed, Activation, Lambda
from keras.layers.core import Dense
from keras.layers.embeddings import Embedding
from keras.models import Model, Sequential, model_from_yaml, load_model, model_from_json
from keras.optimizers import SGD, Adam, sgd
from keras.preprocessing import sequence
from keras.utils import Sequence, GeneratorEnqueuer, OrderedEnqueuer, to_categorical
from keras.utils.generic_utils import Progbar
from lasagne.init import Constant, Sparse
from lasagne.layers import  MergeLayer, ReshapeLayer, FlattenLayer, ConcatLayer
from lasagne.layers import DenseLayer, InputLayer, batch_norm, DropoutLayer
from lasagne.nonlinearities import rectify, elu, softmax, sigmoid
from lasagne.regularization import regularize_network_params, l1, l2, regularize_layer_params_weighted
from matplotlib.animation import FuncAnimation
from matplotlib.colors import ListedColormap
from matplotlib.lines import Line2D
from numpy.linalg import eig
from numpy.random import randint
from pandas import DataFrame, read_csv
from pynput import keyboard
from random import randint, shuffle, seed, random, choice
from redis import Redis, RedisError
from requests import get
from requests.exceptions import RequestException
from scipy import stats
from scipy.sparse import csr_matrix, diags
from scipy.sparse import csr_matrix, diags, spdiags, identity
from scipy.sparse import linalg
from scipy.spatial import distance
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from sklearn import datasets, linear_model
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.manifold import TSNE
from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from tensorflow.python.ops import ctc_ops as ctc
from time import time
import PyPDF2 as pdf
import collections
import copy
import datetime
import functools
import googlemaps
import gym
import h5py
import ipdb
import json
import keras.backend as K
import lasagne
import math
import matplotlib
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import networkx as nx
import numpy as np
import os
import pandas as pd
import pickle
import random
import re
import requests
import scipy
import scipy.io as sio
import seaborn as sns
import socket
import sys
import tensorflow as tf
import theano
import theano.tensor as T
import time
import unicodedata
import unittest
import warnings

plt.close('all')

who/whos        - check the variables in memory for ipython/jupyter
pylint  -r n toto.py    - checks errors / style imperfections in the code
yapf toto.py > toto_clean.py           # automate python formatting (pip install yapf)
# LINKS
# =============================================================================
https://jsfiddle.net/boilerplate/jquery # to learn javascript, CSS, html, bref web stuff
http://pandas.pydata.org/pandas-docs/stable/user_guide/cookbook.html # pandas help and useful commands
http://google.github.io/styleguide/pyguide.html # Google python style guide
https://docs.pytest.org/en/latest/ # Tests in python (framework)
http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx/specials/unittest_coverage_git_profling.html # tests avec Xavier Dupre
https://docs.python.org/3/library/unittest.html # Tests on python software fundation
http://www.labri.fr/perso/nrougier/teaching/numpy.100/ # 100 code examples on nympy
https://codingcompetitions.withgoogle.com/codejam # Google problem solving competition
http://www.xavierdupre.fr/app/teachpyx/helpsphinx/c_classes/classes.html#presentation-des-classes-methodes-et-attributs

# BASES
# =============================================================================
first_var, *rest = ['toto', 'titi', 'tutu'] # And vice versa #toto
z = {**x, **y}              # To join dictionnaries together
d = {}; d['a'] = d.get('a', 0) # To initialize a dict entry without if or try
https://wiki.python.org/moin/TimeComplexity # about complexity in python
cles, vals = zip(*dic_toto.items()) # to explode a dict into its keys and values lists
http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx/notebooks/matrix_dictionary.html  # produit matriciel sparse matrix (a la main)
os.listdir('.')     # to list the files in the given directory


# MODULES, PACKAGES
# =============================================================================
__init__.py         # file (can be empty) in every directory supposed to be a package
np.__name__         # returns the name of the module as a string
__all__ = ['truc', 'chose'] # in __init__.py, lists what modules to be loaded when from toto import * is summoned
from . or .. import toto  # relative paths to load a module (only in a module different from the main module, since name is always '__main__')


# ERRORS and EXCEPTIONS   (https://docs.python.org/3/tutorial/errors.html)
# =============================================================================
# At least 2 distinguishable kinds of errors :
# SYNTAX errors (= parsing errors) - Ex :
if True print('syntax error') # miss colon
# EXCEPTIONS, even when syntactically correct, might have an error at EXECUTION
# when not handled, result in error messages like :
>>> 10 * (1/0)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
ZeroDivisionError: integer division or modulo by zero # They come in different types
# Standard exception names are built-in identifiers (not reserved keywords)
# Handling exceptions : if an exception occurs in try statement, it's either
# caught by except below (if type of the error matches the one of the except)
# or it is passed on to the outer try statements. If none is found, then it is
# an "unhandled exception" and execution stops  with a message
while True:
    try:
        x = int(input("Please enter a number: "))
        break
    except ValueError as e:
        print("Value error: {0}".format(e))

try:
    f = open('myfile.txt')
    s = f.readline()
    i = int(s.strip())
except OSError as err:
    print("OS error: {0}".format(err))
except ValueError:
    print("Could not convert data to an integer.")
except: # If not caught by other excepts, goes here, but careful not to hide any real bug, so it's better to raise it again after that message (helpful to debug ??)
    print("Unexpected error:", sys.exc_info()[0])
    raise
else: # Executed if no exceptions caught, better here than in the try statement
# if we don't want errors of that portion of the code to be caught by the
# previous try statement
    print("rest of the code if no exception caught")
finally: # Clean up actions : executed under all circumstances
    print('Always salute : bye bye')

# RAISING exceptions
# to force a specified exception to occur, the sole argument to raise must be
# an exception instance or an exception class (or class derived from Exception)
raise ValueError # shorthand for 'raise ValueError()'
raise NameError('HiThere')

# USER-DEFINED exceptions
class Error(Exception):
    """Base class for exceptions in this module."""
    pass

class InputError(Error):
    """Exception raised for errors in the input.

    Attributes:
        expression -- input expression in which the error occurred
        message -- explanation of the error
    """

    def __init__(self, expression, message):
        self.expression = expression
        self.message = message

class TransitionError(Error):
    """Raised when an operation attempts a state transition that's not
    allowed.

    Attributes:
        previous -- state at beginning of transition
        next -- attempted new state
        message -- explanation of why the specific transition is not allowed
    """

    def __init__(self, previous, next, message):
        self.previous = previous
        self.next = next
        self.message = message

# ASSERTIONS exceptions
assert ('linux' in sys.platform), "This code runs on Linux only." # pass if True, else raise an AssertionError : verifies if a certain condition is met and throw an exception if it isn't


# COMPILED vs INTERPRETED (vs HYBRID)
# =============================================================================
"Compiled (C, C++, Objective-C)" # --> reduced to machine-specific intructions
# before being saved as executable file (less portable as a result, but
# potentially more efficient)

"Interpreted (PHP, JavaScript)" # reduced to machine instructions on the fly at
# runtime : more portable, less efficient, code available when sharing

"Hybrid (Java, C#, VB.NET, Python)" # first transform into a middle ground
# language before interpreting


# INTROSPECTION - study objects at runtime
# =============================================================================
id(toto)            # unique identifier of a python object
dir(toto)           # returns all the methods/attributs of a given class/object
help(toto)          # displays help for the object (all methods, attributes etc)
type(toto)          # returns the type of an object


# FUNCTIONS - LAMBDA
# =============================================================================
def function_name(arg1, arg2="initialisation de l arg"):
    return resultat_func

print((lambda x:x**2 + 5x)(4))


# CONTROL STRUCTURES
# ============================================================================
for i in range(7):
for f, v, m in zip(fruits, vegetables, meat): # loop over several iterables
if condition1: 
elif condition2:
else:
while condition:
    break # Si on veut forcer la sortie de la boucle

# Switch (inexistant in python) with a dictionary
def f(x):
    return {
        'a': 1,
        'b': 2
    }.get(x, 9)    # 9 is default if x not found
# Or if you want cases more complicated
# define the function blocks
def zero():
    print "You typed zero.\n"
def sqr():
    print "n is a perfect square\n"
def even():
    print "n is an even number\n"
def prime():
    print "n is a prime number\n"
# map the inputs to the function blocks
options = {0 : zero,
           1 : sqr,
           4 : sqr,
           9 : sqr,
           2 : even,
           3 : prime,
           5 : prime,
           7 : prime,
}
# Invoking of the switch
options[num]()

# Decorator
@timer # A mettre au dessus de la definition de la fonction a decorer
def timer(func):# Decorator pour mesurer le temps mis par une fonction pour s executer
    def f(*args, **kwargs):
        before = time()
        rv = func(*args, **kwargs)
        after = time()
        print('Temps ecoule : ', after - before)
        return rv
    return f

@contextmanager
def timer(name):
    t0 = time()
    print('\nstarting :', name)
    yield
    if time() - t0 < 120:
      print('done in', round(time() - t0, 1),  's')
    else:
      print('done in ', round((time() - t0)/60, 1), 'm')

 with timer('Label Encoding'): # The context manager enables to monitor the application of the timer with indentation
    for c in cat_cols:
      df[c] = LabelEncoder().fit_transform(df[c].values.astype('str'))

# Generator - yield/send
import random

def get_data():
    """Return 3 random integers between 0 and 9"""
    return random.sample(range(10), 3)

def consume():
    """Displays a running average across lists of integers sent to it"""
    running_sum = 0
    data_items_seen = 0

    while True:
        data = yield
        data_items_seen += len(data)
        running_sum += sum(data)
        print('The running average is {}'.format(running_sum / float(data_items_seen)))

def produce(consumer):
    """Produces a set of values and forwards them to the pre-defined consumer
    function"""
    while True:
        data = get_data()
        print('Produced {}'.format(data))
        consumer.send(data)
        yield

if __name__ == '__main__':
    consumer = consume()
    consumer.send(None)
    producer = produce(consumer)

    for _ in range(10):
        print('Producing...')
        next(producer)


# GRAPHES
# =============================================================================
# Refs :
# matplotlib, seaborn, networkx, bokeh, bqplot, flexx...
graph = {'A': set(['B', 'C']),
         'B': set(['A', 'D', 'E']),
         'C': set(['A', 'F']),
         'D': set(['B']),
         'E': set(['B', 'F']),
         'F': set(['C', 'E'])}
# Object - class graph
class Graph:
    def __init__(self, size):
        self.size = size
        self.vertices = [[]for i in range(size)]
    def add_edge(self, i, j):
        if not(i in self.vertices[j]):
            self.vertices[j].append(i)

# Depth First Search implementations
def dfs(graph, start): # using stack
    visited, stack = set(), [start]
    while stack:
        vertex = stack.pop()
        if vertex not in visited:
            visited.add(vertex)
            stack.extend(graph[vertex] - visited)
    return visited
dfs(graph, 'A') # {'E', 'D', 'F', 'A', 'C', 'B'}

def dfs(graph, start, visited=None): # Recursive
    if visited is None:
        visited = set()
    visited.add(start)
    for next in graph[start] - visited:
        dfs(graph, next, visited)
    return visited
dfs(graph, 'C') # {'E', 'D', 'F', 'A', 'C', 'B'}

# Paths
def dfs_paths(graph, start, goal): # using stack and generator
    stack = [(start, [start])]
    while stack:
        (vertex, path) = stack.pop()
        for next in graph[vertex] - set(path):
            if next == goal:
                yield path + [next]
            else:
                stack.append((next, path + [next]))
list(dfs_paths(graph, 'A', 'F')) # [['A', 'C', 'F'], ['A', 'B', 'E', 'F']]

def dfs_paths(graph, start, goal, path=None): # Recursive and yield
    if path is None:
        path = [start]
    if start == goal:
        yield path
    for next in graph[start] - set(path):
        yield from dfs_paths(graph, next, goal, path + [next])
list(dfs_paths(graph, 'C', 'F')) # [['C', 'F'], ['C', 'A', 'B', 'E', 'F']]

def bfs_paths(graph, start, goal):
    queue = [(start, [start])]
    while queue:
        (vertex, path) = queue.pop(0)
        for next in graph[vertex] - set(path):
            if next == goal:
                yield path + [next]
            else:
                queue.append((next, path + [next]))
list(bfs_paths(graph, 'A', 'F')) # [['A', 'C', 'F'], ['A', 'B', 'E', 'F']]

def shortest_path(graph, start, goal):
    try:
        return next(bfs_paths(graph, start, goal))
    except StopIteration:
        return None
shortest_path(graph, 'A', 'F') # ['A', 'C', 'F']

# Breadth First Search
def bfs(graph, start):
    visited, queue = set(), [start]
    while queue:
        vertex = queue.pop(0)
        if vertex not in visited:
            visited.add(vertex)
            queue.extend(graph[vertex] - visited)
    return visited
bfs(graph, 'A') # {'B', 'C', 'A', 'F', 'D', 'E'}


# CLASS & OBJECTS
# =============================================================================
class Voiture: # Convention : class name with just alphanum characters : VoitureSport
	def __init__(self): # method called at the creation of the object
		self.nom = "Ferrari"
ma_voiture = Voiture() # Create an object
ma_voiture.modele = "250"
    @property
    def roues(self):
        print "Récupération du nombre de roues"
        return self._roues

    @roues.setter
    def roues(self, v):
        print "Changement du nombre de roues"
        self._roues  =  v
dir(ma_voiture) # to check the methods of an object
ma_voiture.__dict__ # to check the attributes of an object


# DEBUGGING
# =============================================================================
pdb # toggles the interactive ipython debugger
debug # retroactively enters the pdb debugger (after a run command)
import ipdb        ipdb.set_trace() # stops the run at the trace and launch the pdb debugger
vars() # returns the variables which were defined when the error occured and their value


# EFFICIENCY - PERFORMANCE
# =============================================================================
import profile; profile.run('main()') # runs main and returns time spent on all function calls
%run -p script or %prun run script # magic command to call cProfil which returns the time spent on all function calls
%timeit trucAtimer # mesure/calculate time spent for a line set of commands on Ipython
%%timeit blockTrucAtimer # mesure/calculate time spent for a block set of commands on Ipython
import time; t1 = time.time(); duree = time.time() - t1
import time; t1 = time.perf_counter(); duree = time.perf_counter() - t1 # temps local pour comparaison entre méthodes locales (et pas d'un système à l'autre)
"".join(str_list) instead of += loop # string concatenation
new_list = map(func, old_list) instead of looping # to be checked


###############################################################################
# DATA SCIENCE ################################################################
###############################################################################
# NUMPY
# =============================================================================
# Refs :
# 100 Numpy exercises : http://www.labri.fr/perso/nrougier/teaching/numpy.100/
np.zeros_like(arr) / np.ones_like(arr) # copies shape of a matrix filled with 0/1
np.reshape(arr, (new_size, -1), order='C') # -1 as a jocker, else int size, order to precise what represents the tuple (col, row) or (row, col)
m = np.matrix(' 1 2 3; 4 5 6; 7 8 9')
motif = [[0, 1, 2], [3, 4, 5]]
np.tile(motif, (2, 3)) # Create an array by repeating a pattern


# PANDAS
# =============================================================================
# Dataframe ~ excel sheet/SQL table, quick below 10 millions rows (~10Mo)
#       --> to manipulate blocks (subtable)


# IMPORT DATA
# =============================================================================
df = pd.read_csv("./CTGSimple.csv", sep="\t", header=1)
# See 'iterator' param if the file is too big to be processed in one piece, ex :
for df in pandas.read_csv("iris.txt", sep="\t", iterator=True, chunksize=60):
    print(df.shape)

# EXPLORE DATA
# =============================================================================
numberic_features=df.select_dtypes(include=[np.number])
categorical_features=df.select_dtypes(include=[np.object])

# OPTIMISATION
# =============================================================================
# see scipy.optimize and
http://scipy-lectures.org/advanced/mathematical_optimization/index.html


# MISCELLANEOUS
# =============================================================================
# keyboard listener
from pynput import keyboard
import time
def on_press(key):
    try:
        k = key.char  # single-char keys
    except:
        k = key.name  # other keys
    if key == keyboard.Key.esc:
        return False  # stop listener
    # if k in ['1', '2', 'left', 'right']:  # keys interested
    log.append((time.time(), k)) # store it in global-like variable
log = []
lis = keyboard.Listener(on_press=on_press)
lis.start()  # start to listen on a separate thread
lis.join()  # no this if main thread is polling self.keys

# Count stuff
from collections import Counter
ens = ["a", "b", "gh", "er", "b", "gh"]
hist = Counter(ens)
# Split the dataset, stratify to keep the global proportion of Y categories (if any)
from sklearn.model_selection import train_test_split
Xa, Xv, Ya, Yv = train_test_split(X, Y, shuffle=True, test_size=0.2, stratify=Y) 
# Normalize data
sc=StandardScaler(with_mean=True, with_std=True)
sc=sc.fit(Xav)
Xav=sc.transform(Xav)
#%% #### Trace des courbes d'erreur d'apprentissage
plt.figure()
plt.semilogx(vectC, err_val, color='green', linestyle='--', marker='s', markersize=5, label='Validation') 
plt.semilogx(vectC, err_app, color='blue', linestyle='--', marker='s', markersize=5, label='Apprentissage')
plt.xlabel('Parametre C')
plt.ylabel('Erreur Classification')
plt.legend(loc='best')
plt.show()

# Cross validation (cross_val_score)
neigh = KNeighborsClassifier(n_neighbors=i)
np.mean(cross_val_score(neigh, X, y, cv=5)) # give the mean of the five results of cross_val



# ACP - PCA (dimension reduction - Principal Component Analyse)
# =============================================================================
pca = PCA(n_components=n) # n = number of variable to keep
X_red = pca.fit_transform(X)



# LOGISTIC REGRESSION (optim max(vraisemblance derived from P), with P=f(score) et score=log(P1/P2))
# ============================================================================
# Xa, Xv, Xt, C to be determined
from sklearn import linear_model
from sklearn.metrics import accuracy_score
clf_reglog = linear_model.LogisticRegression(tol=1e-5, multi_class='multinomial', solver='lbfgs') # liblinear (to use one against all method)
vectC = np.logspace(-3, 2, 15)
err_app = np.empty(vectC.shape[0])
err_val = np.empty(vectC.shape[0])
for ind_C, C in enumerate(vectC):
    clf_reglog.C = C
    clf_reglog.fit(Xa, Ya)
    err_val[ind_C] = 1 - accuracy_score(Yv, clf_reglog.predict(Xv))
    err_app[ind_C] = 1 - accuracy_score(Ya, clf_reglog.predict(Xa))
err_min_val, ind_min = err_val.min(), err_val.argmin() # Choix du meilleur C
Copt = vectC[ind_min]
# Model final
clf_reglog.C = Copt
clf_reglog.fit(Xa, Ya)
print('Valeur de Copt = {}'.format(Copt ))
print('Err validation correspondante = {}'.format(100*(1 - accuracy_score(Yv, clf_reglog.predict(Xv)))))
print('Err apprentissage correspondante = {}'.format(100*(1 - accuracy_score(Ya, clf_reglog.predict(Xa)))))
proba = clf_reglog.predict_proba(Xv) # to get the probabilities calculated


# LDA / QDA : Generative method, bayesien theorem, qualitative labels (require knowledge of proba law)
# =============================================================================
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
# LDA
clf_lda = LinearDiscriminantAnalysis(solver='svd', store_covariance=True)
clf_lda.fit(X, Y)
Y_lda = clf_lda.predict(X)
err_lda = sum(Y_lda != Y)/Y.size
print('LDA : taux d''erreur = {}%'.format(100*err_lda))
# QDA
clf_qda = QuadraticDiscriminantAnalysis(store_covariance=True)
clf_qda.fit(X, Y)
print(clf_qda.means_)
Y_qda = clf_qda.predict(X)
err_qda = sum(Y_qda!= Y)/Y.size
print('QDA : taux d''erreur = {}%'.format(100*err_qda))


# K-NEAREST NEIGBORS
# =============================================================================
neigh = KNeighborsClassifier(n_neighbors=i)
neigh.fit(X_train, y_train)
y_pred = neigh.predict(X_val)
res = sum(y_val == y_pred)/len(y_val)*100


# KMEANS - MINIBATCH KMeans
# =============================================================================
# KMeans
model = KMeans(n_X)
plt.scatter(X[:,0], X[:,1], c = model.fit_predict(X))
cal_har_x = metrics.calinski_harabaz_score(X, model.fit_predict(X))
# MiniBatch
model = MiniBatchKMeans(n_X)
plt.scatter(X[:,0], X[:,1], c = model.fit_predict(X))


# HIERARCHICAL CLASSIFICATION
# =============================================================================
model = AgglomerativeClustering(n) # n = number of cluster to find (see sklearn for more)
plt.scatter(X[:,0], X[:,1], c = model.fit_predict(X))


# PLOTs
# =============================================================================
# Geographical maps : see folium par ex
# change plot style
plt.style.use('ggplot')
# To plot an image stored as a vector
def plotimage(x, larg=28, haut=28, title=""):
    img = np.reshape(x, (larg,haut))
    imgplot = plt.imshow(img,cmap='gray')
    plt.title(title, size=12)
    plt.show()
# Plot several charts / figures as a matrix
plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')
plt.subplot(121) # nb ligne, nb col, num fig
plt.scatter(X[:,0], X[:,1], s=100, c=y) # Nuage point, surface, couleur
plt.tight_layout() # to minimize the overlap when drawing plots side by side
plt.show

# Trace de frontiere de decision en 2D
def plot_regions_decision_2d(X, y, classifier, resolution=0.02, titre=' '):

    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])

    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 0, X[:, 0].max() + 0
    x2_min, x2_max = X[:, 1].min() - 0, X[:, 1].max() + 0
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                         np.arange(x2_min, x2_max, resolution))
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    plt.figure()
    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())

    # plot class samples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],
                    alpha=0.6, c=cmap(idx),
                    marker=markers[idx], label= 'classe {}'.format(cl))
    plt.legend(loc='best')
    plt.title(titre, fontsize=12)
#%% ### Trace de la frontiere de decision en 2D
# Utilisation de 2 variables choisies parmi les 4. 
#On fait le modele LDA et QDA que pour ces variables
variables = [2, 3]
## on ne fait ceci que pour le trace de la frontiere de decision de la LDA et QDA en 2D
classifieur = 'LDA'
clf_lda.fit(X[:,variables], Y) 
plot_regions_decision_2d(X[:,variables], Y, clf_lda, 0.02, titre='LDA')
clf_qda.fit(X[:,variables], Y) 
plot_regions_decision_2d(X[:,variables], Y, clf_qda, 0.02, titre='QDA')
plt.show()

def investing(years=30, apport=10000, taux=0.05, savings=5000):
    pesos = apport * (1+taux) + savings
    if years == 1:
        return pesos
    else:
        return investing(years-1, pesos, taux, savings)


			Complexity of Python Operations


In this lecture we will learn the complexity classes of various operations on
Python data types. Then we wil learn how to combine these complexity classes to
compute the complexity class of all the code in a function, and therefore the
complexity class of the function. This is called "static" analysis, because we
do not need to run any code to perform it (contrasted with Dynamic or Emperical
Analysis, when we do run code and take measurements of its execution).

------------------------------------------------------------------------------

Python Complexity Classes

In ICS-46 we will write low-level implementations of all of Python's data types
and see/understand WHY these complexity classes apply. For now we just need to
try to absorb (not memorize) this information, with some -but minimal- 
justification.

Binding a value to any name (copying a refernce) is O(1). Simple operators on
integers (whose values are small: e.g., under 12 digits) like + or == are also
O(1). You should assume small integers in problems unless explicitly told
otherwise.

In all these examples, N = len(data-structure). The operations are organized by
increasing complexity class

Lists:
                               Complexity
Operation     | Example      | Class         | Notes
--------------+--------------+---------------+-------------------------------
Index         | l[i]         | O(1)	     |
Store         | l[i] = 0     | O(1)	     |
Length        | len(l)       | O(1)	     |
Append        | l.append(5)  | O(1)	     | mostly: ICS-46 covers details
Pop	      | l.pop()      | O(1)	     | same as l.pop(-1), popping at end
Clear         | l.clear()    | O(1)	     | similar to l = []

Slice         | l[a:b]       | O(b-a)	     | l[1:5]:O(l)/l[:]:O(len(l)-0)=O(N)
Extend        | l.extend(...)| O(len(...))   | depends only on len of extension
Construction  | list(...)    | O(len(...))   | depends on length of ... iterable

check ==, !=  | l1 == l2     | O(N)          |
Insert        | l[a:b] = ... | O(N)	     | 
Delete        | del l[i]     | O(N)	     | depends on i; O(N) in worst case
Containment   | x in/not in l| O(N)	     | linearly searches list 
Copy          | l.copy()     | O(N)	     | Same as l[:] which is O(N)
Remove        | l.remove(...)| O(N)	     | 
Pop	      | l.pop(i)     | O(N)	     | O(N-i): l.pop(0):O(N) (see above)
Extreme value | min(l)/max(l)| O(N)	     | linearly searches list for value
Reverse	      | l.reverse()  | O(N)	     |
Iteration     | for v in l:  | O(N)          | Worst: no return/break in loop

Sort          | l.sort()     | O(N Log N)    | key/reverse mostly doesn't change
Multiply      | k*l          | O(k N)        | 5*l is O(N): len(l)*l is O(N**2)

Tuples support all operations that do not mutate the data structure (and they
have the same complexity classes).


Sets:
                               Complexity
Operation     | Example      | Class         | Notes
--------------+--------------+---------------+-------------------------------
Length        | len(s)       | O(1)	     |
Add           | s.add(5)     | O(1)	     |
Containment   | x in/not in s| O(1)	     | compare to list/tuple - O(N)
Remove        | s.remove(..) | O(1)	     | compare to list/tuple - O(N)
Discard       | s.discard(..)| O(1)	     | 
Pop           | s.pop()      | O(1)	     | popped value "randomly" selected
Clear         | s.clear()    | O(1)	     | similar to s = set()

Construction  | set(...)     | O(len(...))   | depends on length of ... iterable
check ==, !=  | s != t       | O(len(s))     | same as len(t); False in O(1) if
      	      	     	       		       the lengths are different
<=/<          | s <= t       | O(len(s))     | issubset
>=/>          | s >= t       | O(len(t))     | issuperset s <= t == t >= s
Union         | s | t        | O(len(s)+len(t))
Intersection  | s & t        | O(len(s)+len(t))
Difference    | s - t        | O(len(s)+len(t))
Symmetric Diff| s ^ t        | O(len(s)+len(t))

Iteration     | for v in s:  | O(N)          | Worst: no return/break in loop
Copy          | s.copy()     | O(N)	     |

Sets have many more operations that are O(1) compared with lists and tuples.
Not needing to keep values in a specific order in a set (while lists/tuples
require an order) allows for faster implementations of some set operations.

Frozen sets support all operations that do not mutate the data structure (and
they have the same  complexity classes).


Dictionaries: dict and defaultdict
                               Complexity
Operation     | Example      | Class         | Notes
--------------+--------------+---------------+-------------------------------
Index         | d[k]         | O(1)	     |
Store         | d[k] = v     | O(1)	     |
Length        | len(d)       | O(1)	     |
Delete        | del d[k]     | O(1)	     |
get/setdefault| d.get(k)     | O(1)	     |
Pop           | d.pop(k)     | O(1)	     | 
Pop item      | d.popitem()  | O(1)	     | popped item "randomly" selected
Clear         | d.clear()    | O(1)	     | similar to s = {} or = dict()
View          | d.keys()     | O(1)	     | same for d.values()

Construction  | dict(...)    | O(len(...))   | depends # (key,value) 2-tuples

Iteration     | for k in d:  | O(N)          | all forms: keys, values, items
	      	      	       		     | Worst: no return/break in loop
So, most dict operations are O(1).

defaultdicts support all operations that dicts support, with the same
complexity classes (because it inherits all those operations); this assumes that
calling the constructor when a values isn't found in the defaultdict is O(1) -
which is true for int(), list(), set(), ... (the things we commonly use)

Note that for i in range(...) is O(len(...)); so for i in range(1,10) is O(1).
If len(alist) is N, then

  for i in range(len(alist)):

is O(N) because it loops N times. Of course even 

  for i in range (len(alist)//2):

is O(N) because it loops N/2 times, and dropping the constant 1/2 makes
it O(N): the work doubles when the list length doubles. By this reasoning,

  for i in range (len(alist)//1000000):

is O(N) because it loops N/1000000 times, and dropping the constant 1000000
makes  it O(N): the work doubles when the list length doubles. Remember, we
are interested in what happens as N -> Infinity.

Finally, when comparing two lists for equality, the complexity class above
shows as O(N), but in reality we would need to multiply this complexity class by
O==(...) where O==(...) is the complexity class for checking whether two values
in the list are ==. If they are ints, O==(...) would be O(1); if they are
strings, O==(...) in the worst case it would be O(len(string)). This issue
applies any time an == check is done. We mostly will assume == checking on
values in lists is O(1): e.g., checking ints and small/fixed-length strings.

------------------------------------------------------------------------------

Composing Complexity Classes: Sequential and Nested Statements

In this section we will learn how to combine complexity class information about
simple operations into complexity class information about complex operations
(composed from simple operations). The goal is to be able to analyze all the
statements in a functon/method to determine the complexity class of executing
the function/method. As with computing complexity classes themselves, these
rules are simple and easy to apply once you understand how to use them.

------------------------------------------------------------------------------

Law of Addition for big-O notation

 O(f(n)) + O(g(n)) is O( f(n) + g(n) )

That is, we when adding complexity classes we bring the two complexity classes
inside the O(...). Ultimately, O( f(n) + g(n) ) results in the bigger of the two
complexity class (because we alwasy drop the lower-complexity added term). So,

O(N) + O(Log N)  =  O(N + Log N)  =  O(N)

because N is the faster growing term: lim (N->infinity) Log N/N = 0.

This rule helps us understand how to compute the complexity class of doing any
SEQUENCE of operations: executing a statement that is O(f(n)) followed by
executing a statement that is O(g(n)). Executing both statements SEQUENTIALLY
is O(f(n)) + O(g(n)) which is O( f(n) + g(n) ) by the rule above.

For example, if some function call f(...) is O(N) and another function call
g(...) is O(N Log N), then doing the sequence

   f(...)
   g(...)

is O(N) + O(N Log N) = O(N + N Log N) =  O(N Log N). Of course, executing the
sequence (calling f twice)

  f(...)
  f(...)

is O(N) + O(N) which is O(N + N) which is O(2N) which is O(N) because we discard
multiplicative constants in big-O notation.

Note that an if statment sequentially evaluates test AND THEN one of the blocks.

  if test:    	 assume complexity class of computing test is O(T)
     block 1     assume complexity class of executing block 1 is O(B1)
  else:
     block 2     assume complexity class of executing block 2 is O(B2)

The complexity class for the if is O(T) + max(O(B1),O(B2)). The test is always
evaluated, and one of the blocks is always executed afterward (so, a sequence
of evaulating a test followed by executing a block). In the worst case, the if
will execute the block with the largest complexity class. So, given

  if test:    	 complexity class is O(N)
     block 1     complexity class is O(N**2)
  else:
     block 2     complexity class is O(N)

The complexity class for the if is O(N) + max (O(N**2),O(N))) = O(N) + O(N**2)
= O(N + N**2) = O(N**2).

If the test had complexity class O(N**3), then the complexity class for the if
is O(N**3) + max (O(N**2),O(N))) = O(N**3) + O(N**2) = O(N**3 + N**2) = O(N**3).

In fact, the complexity class for an if can also be written as
O(T) + O(B1) + O(B2): for the if above example O(N) + O(N**2) + O(N) = O(N**2).
Why? Because we always throw away the lower-order terms, whic is like taking
the max of the terms. I prefer writing O(T) + max(O(B1),O(B2)) because it looks
like what is happening: the test is always evaluated, and one of the blocks.

------------------------------------------------------------------------------

Law of Multiplcation for big-O notation

 O(f(n)) * O(g(n)) is O( f(n) * g(n) )

If we repeat an O(f(N)) process O(N) times, the resulting complexity class is
O(N)*O(f(N)) = O( N*f(N) ). An example of this is, if some function call f(...)
is O(N**2), then executing that call N times (in the following loop)

  for i in range(N):
    f(...)

is O(N)*O(N**2) = O(N*N**2) = O(N**3)

This rule helps us understand how to compute the complexity class of doing some 
statement INSIDE A BLOCK controlled by a statement that is REPEATING it. We
multiply the complexity class of the number of repetitions by the complexity
class of the statement (sequence; using the summing rule) being repeated.

Compound statements can be analyzed by composing the complexity classes of
their constituent statements. For sequential statements (including if tests and
their block bodies) the complexity classes are added; for statements repeated
in a loop the complexity classes are multiplied.

------------------------------------------------------------------------------

One Function Specification/3 Implementations and their Analysis

Let's use the data and tools discussed above to analyze (determine the
complexity classes) of three different functions that each compute the same
result: whether or not a list contains only unique values (no duplicates). We
will assume in all three examples that len(alist) is N and that we can compare
the list elements in O(1): e.g., they are small ints or strs.

1) Algorithm 1: A list is unique if each value in the list does not occur in any
later indexes: alist[i+1:] is a list slice containing all values after the one
at index i.

def is_unique1 (alist : [int]) -> bool:
    for i in range(len(alist)):		O(N) - for every index; see * below
        if alist[i] in alist[i+1:]:	O(N) - index+add+slice+in: O(1)+O(1)+O(N)+O(N) = O(N)
            return False		O(1) - never executed in worst case; ignore
    return True				O(1) - always executed in worst case; use

*Note that creating a range object requires 3 sequential operations: computing
the arguments, passing the arguments to __init__, and executing the body of
__init__. The latter two are both O(1), and computing len(alist) is also O(1),
so the complexity of range(len(alist)) is O(1)+O(1)+O(1) = O(1).

The complexity class for executing the entire function is O(N) * O(N) + O(1)
= O(N**2). So we know from the previous lecture that if we double the length of
alist, this function takes 4 times as long to execute.

-----
Many students want to write this as O(N) * ( O(N) + O(1) ) + O(1) because the
if statement's complexity is O(N) + O(1): complexity of test + complexity of
block when test is True (there is no block when test is False). But in the
WORST CASE, the return is NEVER EXECUTED (the loop keeps executing) so it
should not appear in the formula. Although, even if it appears in this formula,
the formula still computes the same complexity class (because O(N) + O(1) is
still O(N)): O(N**2).

So, in the worst case, we never return False and keep executing the loop, so
this O(1) does not appear in the formula. Also, in the worst case the list
slice is aliset[1:] which is O(N-1) = O(N), although when i is len(alist) the
slice contains 0 values: is empty. The average list slice taken in the if has
N/2 values, which is still O(N).
-----

We can also write this function purely using loops (no slicing), but the
complexity class is the same.

def is_unique1 (alist : [int]) -> bool:
    for i in range(len(alist)):		O(N) - for every index
        for j in range(i+1,len(alist)): O(N) - N-i indexes; O(N) in worst case
            if alist[i] == a[j]:	O(1) - index+index+==: O(1)+O(1)+O(1) = O(1)
                return False		O(1) - never executed in worst case; ignore
    return True				O(1) - always executed in worst case; use

The complexity class for executing the entire function is O(N)*O(N)*O(1) + O(1)
= O(N**2). So we know from the previous lecture that if we double the length of
alist, this function takes 4 times as long to execute.

------
2) Algorithm 2: A list is unique if when we sort its values, no ADJACENT values
are equal. If there were duplicate values, sorting the list would put these
duplicate values right next to each other (adjacent). Here we copy the list so
as to not mutate (change the order of) the parameter's list by sorting it
(functions generally shouldn't mutate their arguments unless that is the purpose
of the function): it turns out that copying the list does not increase the
complexity class of the method, because the O(N) used for copying is not the
largest added term computing the complexity class of this function's body.

def is_unique2 (alist : [int]) -> bool:
    copy = list(alist)			O(N)
    copy.sort()				O(N Log N) - for fast Python sorting
    for i in range(len(alist)-1):	O(N) - really N-1, but that is O(N); len and - are both O(1)
        if copy[i] == copy[i+1]:	O(1): +, 2 [i], and  == on ints: all O(1)
            return False		O(1) - never executed in worst case
    return True	   			O(1) - always executed in worst case

The complexity class for executing the entire function is given by the sum
O(N) + O(N Log N) + O(N)*O(1) + O(1) = O(N + N Log N + O(N*1) + 1) =
O(N + N Log N + N + 1) = O(N Log N + 2N + 1) = O(N Log N). So the complexity
class for this algorithm/function is lower than the first algorithm, the
is_unique1 function. For large N unique2 will eventually run faster. Because
we don't know the constants, we don't know which is faster for small N.

Notice that the complexity class for sorting is dominant in this code: it does
most of the work. If we double the length of alist, this function takes a bit
more than twice the amount of time. In N Log N: N doubles and Log N gets a tiny
bit bigger (i.e., Log 2N = 1 + Log N; e.g., Log 2000 = 1 + Log 1000 = 11, so
compared to 1000 Log 1000, doubling N is 2000 Log 2000, which is just 2.2 times
bigger, or 10% bigger than just doubling).

Looked at another way if T(N) = c*(N Log N), then T(2N) = c*(2N Log 2N) =
c*2N(Log N + 1) = c*2N Log N + c*2N = 2*T(N) + c*2N. Or, computing the doubling signature

  T(2N)     c*2N Log N + c*2N     c*2N Log N       c*2N              2
-------- = ------------------- = ------------ + ----------- = 2 + -------
  T(N)         c N Log N           c N Log N     c N Log N         Log N

So, the ratio is 2 + a bit (and that bit gets smaller -very slowly- as N
increases): for N >= 10**3 it is <= 2.2; for N >= 10**6 it is <= 2.1; for N >=
10**9 it it < 2.07. So, it is a bit worse than doubling each time, but much
better than O(N**2) which is quadrupling each time.

In fact, we could also simplify

    copy = list(alist)			O(N)
    copy.sort()				O(N Log N) - for fast Python sorting

to just

    copy = sorted(alist)                O(N Log N) - for fast Python sorting

because sorted will create a list of all the values in its iterable argument,
and return it after mutating (sorting) it. So we don't have to explicitly
create such a copy in our code.

This change will speed up the code, but it won't change the complexity analysis
because O(N + N Log N) = O (N Log N). Speeding up code is always good, but
finding an algorithm in a better complexity class (as we did going from
is_unique1 to is_unique2) is much btter

Finally, is_unique2 works only if all the values in the list are comparable
(using the < relational operator needed for sorting): it would fail if the list
contained both integers and strings. Whereas, using either version of
is_unique1, requires only comparing values with ==: 3 == 'xyz' is False; it
does not raise an exception.

------
3) Algorithm 3: A list is unique if when we turn it into a set, its length is
unchanged: if duplicate values were added to the set, its length would be
smaller than the length of the list by exactly the number of duplicates in the
list added to the set.

def is_unique3 (alist : [int]) -> bool:
    aset = set(alist)			O(N): construct set from alist values
    return len(aset) == len(alist)	O(1): 2 len (each O(1)) and == ints O(1)

The complexity class for executing the entire function is O(N) + O(1) =
O(N + 1) = O(N). So the complexity class for this algortihm/function is lower
than both the first and second algorithms/functions. If we double the length of
alist, this function takes just twice the amount of time. We could write the
body of this function more simply as: return len(set(alist)) == len(alist),
where evaluating set(alist) takes O(N) and then computing the two len's and
comparing them for equality are all O(1): O(N)+O(1)+O(1)+O(1) = O(N).

Unlike is_unique2, it can work for lists containing both integers and strings.
But, is_unique3 works only if all the values in the list are hashable/immutable
(a requirement for storing values in a set). So, it would not work for a list of
lists.

So the bottom line here is that there might be many algorithms/functions to
solve some problem. If the function bodies are small, we can analyze them
statically (looking at the code, not needing to run it) to determine their
complexity classes. For large problem sizes, the algorithm/function with the
smallest complexity class will ultimately be best, running in the least amount
of time.

But, for small problem sizes, complexity classes don't determine which is best:
for small problem we need to take into account the CONSTANTS and lower order
terms that we ignored when computing complexity classes). We can run the
functions (dynamic analysis, aka empirical analysis) to test which is fastest
on small problem sizes.

And finally, sometimes we must put additional constraints on data passed to
some implementations: is_unique2 require that its list store values comparable
by < (for sorting it); is_unique3 requires that its list store values that
hashable/immutable.

------------------------------------------------------------------------------

Using a Class (implementable 3 ways) Example:

We will now look at the solution of a few problems (combining operations on a
priority queue: pq) and how the complexity class of the result is affected by
three different classes/implementations of priority queues.

In a priority queue, we can add values to and remove values from the data
structure. A correctly working priority queue always removes the maximum value
remaining in the priority queue (the one with the highest priority). Think of a
line/queue outside of a Hollywood nightclub, such that whenever space opens up
inside, the most famous person in line gets to go in (the "highest priority"
person), no matter how long less famous people have been standing in line
(contrast this with first come/first serve, which is a regular -non priority-
queue; in a regaular queue, whoever is first in the line -has been standing in
line longest- is admitted next).

For the problems below, all we need to know is the complexity class of the
"add" and "remove" operations.

                      add           remove
	         +-------------+-------------+
Implementation 1 |    O(1)     |    O(N)     |
	         +-------------+-------------+
Implementation 2 |    O(N)     |    O(1)     |
	         +-------------+-------------+
Implementation 3 |  O(Log N)   |  O(Log N)   |
	         +-------------+-------------+

Implementation 1 adds the new value into the pq by appending the value at the
rear of a list or the front of a linked list: both are O(1); it removes the
highest priority value by scanning through the list or linked list to find the
highest value, which is O(N), and then removing that value, also O(N) in the
worst case  (removing at the front of a list; at the rear of a linked list).

Implementation 2 adds the new value into the pq by scanning the list or linked
list for the right spot to put it and putting it there, which is O(N). Lists
store their highest priority at the rear (linked lists at the front); it
removes the highest priority value from the rear for lists (or the front for
linked lists), which is O(1).

So Implementations 1 and 2 swap the complexity classes in their add/remove
method. Implementation 1 doesn't keep the values in order: so easy to add but
hard to find/remove the maximum (must scan). Implementation 2 keeps the values
in order: so hard to add (need to scan to find where it goes) but easy to
find/remove the maximum (at one end; the fast one).

Implementation 3, which is discussed in ICS-46, uses a binary heap tree (not a
binary search tree) to implement both operations with "middle" complexity
O(Log N): this complexity class greater than O(1) but less than O(N). Because
Log N grows so slowly, O(Log N) is actually closer to O(1) than O(N) even though
O(1) doesn't grow at all: Log N grows that slowly.

Problem 1: Suppose we wanted to use the priority queue to sort N values: we
add N values in the pq and then remove all N values (first the highest, next
the second highest, ...). Here is the complexity of these combined operations
for each implementation.

Implementation 1: N*O(1) + N*O(N)         = O(N)   + O(N**2)    = O(N**2)
Implementation 2: N*O(N) + N*O(1)         = O(N**2) + O(N)      = O(N**2)
Implementation 3: N*O(Log N) + N*O(Log N) = O(NLogN) + O(NLogN) = O(NLogN)

Note N*O(...) is the same as O(N)*O(...) which is the same as O(N * ...)

Here, Implementation 3 has the lowest complexity class for the combined
operations. Implementations 1 and 2 each do one operation quickly but the other
slowly: both are done O(N) times. The slowest operation determines the
complexity class, and both are equally slow. The complexity class O(Log N) is
between O(1) and O(N); surprisingly, it is actually "closer" to O(1) than O(N),
even though it does grow -because it grows so slowly; yes, O(1) doesn't grow at
all, but O(Log N) grows very slowly: the known Universe has about 10**90
particles of matter, and Log 10**90 = Log (10**3)**30 = 300, which isn't very
big compared to 10**90 (like 86 orders of magnitude less).

Problem 2: Suppose we wanted to use the priority queue to find the 10 biggest
(of N) values: we would enqueue N values and then dequeue 10 values. Here is
the complexity of these combined operations for each implementation..

Implementation 1: N*O(1) + 10*O(N)         = O(N)   + O(N)      = O(N)
Implementation 2: N*O(N) + 10*O(1)         = O(N**2) + O(1)     = O(N**2)
Implementation 3: N*O(Log N) + 10*O(Log N) = O(NLogN) + O(LogN) = O(NLogN)

Here, Implementation 1 has the lowest complexity for the combined operations.
That makes sense, as the operation done N times (add) is very simple (add to
the end of a list/the front of a linked list, where each add is O(1)) and the
operation done a constant number of times (10, independent of N) is the
expensive operation (remove, which is O(N)). It even beats the complexity of
Implementation 3. So, as N gets bigger, implementation 1 will eventually become
faster than the other two for the "find the 10 biggest" task.

So, the bottom line here is that sometimes there is NOT a "best all the time" 
implementation for a data structure. We need to know what problem we are
solving (the complexity classes of all the operations in various
implementations and HOW OFTEN we must do these operations) to choose the most
efficient implementation for solving the problem.
